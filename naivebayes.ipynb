{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_text_file = 'dev_text.txt'\n",
    "dev_label_file = 'dev_label.txt'\n",
    "heldout_text_file = 'heldout_text.txt'\n",
    "heldout_pred_file = 'heldout_pred_nb.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_label_file, 'rt', encoding='UTF8') as f:\n",
    "    dev_label = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "with open(dev_text_file, 'rt', encoding='UTF8') as f:\n",
    "    dev_text = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "with open(heldout_text_file, 'rt', encoding='UTF8') as f:\n",
    "    heldout_text = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "dev_label = [label.replace('\\n', '') for label in dev_label]\n",
    "dev_text = [text.replace('\\n', '') for text in dev_text]\n",
    "heldout_text = [text.replace('\\n', '') for text in heldout_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    word = [] #list of cleaned token \n",
    "    text = [] #list of cleaned text\n",
    "    \n",
    "    for line in data:\n",
    "        content_text = re.sub(r'\\([^)]*\\)', '', line) \n",
    "        sent_text = sent_tokenize(content_text)\n",
    "\n",
    "        normalized_text = []\n",
    "        for string in sent_text:\n",
    "            tokens = re.sub(r'[^A-Za-z0-9\\s]+', '', string.lower())\n",
    "            tokens = re.sub(r'\\d+', '', tokens)\n",
    "            normalized_text.append(tokens)\n",
    "\n",
    "        result_content = ' '.join(normalized_text)\n",
    "        result_sentence = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "        result = [word for sentence in result_sentence for word in sentence]\n",
    "\n",
    "        word.append(result)\n",
    "        text.append(result_content)\n",
    "    \n",
    "    return word, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_word_list, dev_text_list = preprocess(dev_text)\n",
    "heldout_word_list, heldout_text_list = preprocess(heldout_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data into pd.DataFrame format\n",
    "Train_word = pd.DataFrame(columns=['label', 'word'])\n",
    "Train_text = pd.DataFrame(columns=['label', 'text'])\n",
    "for label, text, word in zip(dev_label, dev_text_list, dev_word_list):\n",
    "    Train_word = Train_word.append([{'label': label, 'word': word}], ignore_index=True, sort=False)\n",
    "    Train_text = Train_text.append([{'label': label, 'text': text}], ignore_index=True, sort=False)\n",
    "\n",
    "Test_word = pd.DataFrame(columns=['label', 'word'])\n",
    "Test_text = pd.DataFrame(columns=['label', 'text'])\n",
    "for text, word in zip(heldout_text_list, heldout_word_list):\n",
    "    Test_word = Test_word.append([{'label': None, 'word': word}], ignore_index=True, sort=False)\n",
    "    Test_text = Test_text.append([{'label': None, 'text': text}], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_token_list = [token for text in Train_word['word'] for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocab from pos and neg\n",
    "#5000 each, unique size is 6692\n",
    "pos_token_list = [token for words in Train_word[Train_word['label']=='pos']['word'] for token in words]\n",
    "pos_vocab = Counter(pos_token_list).most_common(5000)\n",
    "pos_vocab = [c[0] for c in pos_vocab]\n",
    "\n",
    "neg_token_list = [token for words in Train_word[Train_word['label']=='neg']['word'] for token in words]\n",
    "neg_vocab = Counter(neg_token_list).most_common(5000)\n",
    "neg_vocab = [c[0] for c in neg_vocab]\n",
    "\n",
    "vocab = np.unique(pos_vocab+neg_vocab)\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use...\n",
       "                vocabulary={'a': 0, 'abandoned': 1, 'abc': 2, 'abducted': 3,\n",
       "                            'ability': 4, 'able': 5, 'abomination': 6,\n",
       "                            'about': 7, 'aboutbr': 8, 'above': 9, 'abr': 10,\n",
       "                            'abroad': 11, 'abrupt': 12, 'absence': 13,\n",
       "                            'absolute': 14, 'absolutely': 15, 'absorbing': 16,\n",
       "                            'absurd': 17, 'absurdity': 18, 'abuse': 19,\n",
       "                            'abused': 20, 'abusing': 21, 'abusive': 22,\n",
       "                            'abysmal': 23, 'academy': 24, 'accent': 25,\n",
       "                            'accents': 26, 'accept': 27, 'acceptable': 28,\n",
       "                            'accepted': 29, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocab_dict, ngram_range=(1,1))\n",
    "vectorizer.fit(train_all_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_feature = vectorizer.transform(Train_text['text'])\n",
    "Test_feature = vectorizer.transform(Test_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(Train_feature.shape[0]):\n",
    "    train_data.append([Train_text['label'].iloc[i], Train_feature.getrow(i)])\n",
    "train_data = np.asarray(train_data)\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_set = train_test_split(train_data, test_size=0.2, random_state=SEED)\n",
    "test_set = Test_feature.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Sampling for train data\n",
    "train_set_pos = train_set[train_set[:,0]=='pos']\n",
    "train_set_neg = train_set[train_set[:,0]=='pos']\n",
    "train_set_sampling = [train_set.copy()]\n",
    "for i in range(10):\n",
    "    pos_sample = train_set_pos[np.random.choice(range(len(train_set_pos)), 100),:]\n",
    "    train_set_sampling.append(pos_sample)\n",
    "\n",
    "    neg_sample = train_set_pos[np.random.choice(range(len(train_set_neg)), 100),:]\n",
    "    train_set_sampling.append(neg_sample)\n",
    "train_set_sampling = np.vstack(train_set_sampling)\n",
    "np.random.shuffle(train_set_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[:,0]\n",
    "X_train = vstack(train_set[:,1]).toarray()\n",
    "y_validation = validation_set[:,0]\n",
    "X_validation = vstack(validation_set[:,1]).toarray()\n",
    "X_test = test_set.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "params = {'alpha':[0.1, 0.5, 1.0, 2.0]}\n",
    "nb = MultinomialNB()\n",
    "gs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "gs.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(heldout_pred_file, 'w') as f:\n",
    "    for l in predict:\n",
    "        f.write(str(l)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_df)",
   "language": "python",
   "name": "conda_df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
