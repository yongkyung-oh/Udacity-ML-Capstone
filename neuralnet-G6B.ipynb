{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import re\n",
    "import random\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for data processing and model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.vocab import Vectors, Vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model\n",
    "#Embedding = 'W2V'\n",
    "Embedding = 'G6B' \n",
    "Model = 'LSTM'\n",
    "\n",
    "# Setup hyper-parameters\n",
    "NUM_WORDS = 1000\n",
    "NUM_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASS = 2\n",
    "EPOCHS = 50\n",
    "\n",
    "# Define model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output = [batch size, sent len, hid dim * num directions]\n",
    "        # hidden = [batch size, num layers * num directions, hid dim]\n",
    "        # cell = [batch size, num layers * num directions, hid dim]\n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        # hidden = [batch size, hid dim * num directions]\n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in train_iter:\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        y.data.sub_(1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(train_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x, y = batch.text.to(device), batch.label.to(device)\n",
    "            y.data.sub_(1)\n",
    "            logit = model(x)\n",
    "            loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "            corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_text_file = 'dev_text.txt'\n",
    "dev_label_file = 'dev_label.txt'\n",
    "heldout_text_file = 'heldout_text.txt'\n",
    "heldout_pred_file = 'heldout_pred_nn_G6B.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_label_file, 'rt', encoding='UTF8') as f:\n",
    "    dev_label = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "with open(dev_text_file, 'rt', encoding='UTF8') as f:\n",
    "    dev_text = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "with open(heldout_text_file, 'rt', encoding='UTF8') as f:\n",
    "    heldout_text = np.asarray(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "dev_label = [label.replace('\\n', '') for label in dev_label]\n",
    "dev_text = [text.replace('\\n', '') for text in dev_text]\n",
    "heldout_text = [text.replace('\\n', '') for text in heldout_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    word = [] #list of cleaned token \n",
    "    text = [] #list of cleaned text\n",
    "    \n",
    "    for line in data:\n",
    "        content_text = re.sub(r'\\([^)]*\\)', '', line) \n",
    "        sent_text = sent_tokenize(content_text)\n",
    "\n",
    "        normalized_text = []\n",
    "        for string in sent_text:\n",
    "            tokens = re.sub(r'[^A-Za-z0-9\\s]+', '', string.lower())\n",
    "            tokens = re.sub(r'\\d+', '', tokens)\n",
    "            normalized_text.append(tokens)\n",
    "\n",
    "        result_content = ' '.join(normalized_text)\n",
    "        result_sentence = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "        result = [word for sentence in result_sentence for word in sentence]\n",
    "\n",
    "        word.append(result)\n",
    "        text.append(result_content)\n",
    "    \n",
    "    return word, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_word_list, dev_text_list = preprocess(dev_text)\n",
    "heldout_word_list, heldout_text_list = preprocess(heldout_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data into pd.DataFrame format\n",
    "Train_word = pd.DataFrame(columns=['label', 'word'])\n",
    "Train_text = pd.DataFrame(columns=['label', 'text'])\n",
    "for label, text, word in zip(dev_label, dev_text_list, dev_word_list):\n",
    "    Train_word = Train_word.append([{'label': label, 'word': word}], ignore_index=True, sort=False)\n",
    "    Train_text = Train_text.append([{'label': label, 'text': text}], ignore_index=True, sort=False)\n",
    "\n",
    "Test_word = pd.DataFrame(columns=['label', 'word'])\n",
    "Test_text = pd.DataFrame(columns=['label', 'text'])\n",
    "for text, word in zip(heldout_text_list, heldout_word_list):\n",
    "    Test_word = Test_word.append([{'label': None, 'word': word}], ignore_index=True, sort=False)\n",
    "    Test_text = Test_text.append([{'label': None, 'text': text}], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random sampling for train data\n",
    "Train_text_pos = Train_text[Train_text['label']=='pos']\n",
    "Train_text_neg = Train_text[Train_text['label']=='neg']\n",
    "Train_text_sampling = Train_text.copy()\n",
    "for i in range(10):\n",
    "    np.random.shuffle(Train_text_pos.values)\n",
    "    pos_sample = Train_text_pos.sample(n=100)\n",
    "    Train_text_sampling = Train_text_sampling.append(pos_sample)\n",
    "\n",
    "    np.random.shuffle(Train_text_neg.values)\n",
    "    neg_sample = Train_text_neg.sample(n=100)\n",
    "    Train_text_sampling = Train_text_sampling.append(neg_sample)\n",
    "np.random.shuffle(Train_text_sampling.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv format to data folder\n",
    "Train_text.to_csv('train.csv', index=False)\n",
    "#Train_text_sampling.to_csv('train.csv', index=False)\n",
    "Test_text.to_csv('test.csv', index=False)\n",
    "\n",
    "# Define train_set / val_set / test_set\n",
    "#train_set_df, validation_set_df = train_test_split(Train_text, test_size=0.2, random_state=SEED)\n",
    "train_set_df, validation_set_df = train_test_split(Train_text_sampling, test_size=0.2, random_state=SEED)\n",
    "test_set_df = Test_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random sampling for train set\n",
    "train_set_df_pos = train_set_df[train_set_df['label']=='pos']\n",
    "train_set_df_neg = train_set_df[train_set_df['label']=='neg']\n",
    "train_set_df_sampling = train_set_df.copy()\n",
    "for i in range(10):\n",
    "    pos_sample = train_set_df_pos.sample(n=80)\n",
    "    train_set_df_sampling = train_set_df_sampling.append(pos_sample)\n",
    "\n",
    "    neg_sample = train_set_df_neg.sample(n=80)\n",
    "    train_set_df_sampling = train_set_df_sampling.append(neg_sample)\n",
    "np.random.shuffle(train_set_df_sampling.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random sampling for validation set\n",
    "validation_set_df_pos = validation_set_df[validation_set_df['label']=='pos']\n",
    "validation_set_df_neg = validation_set_df[validation_set_df['label']=='neg']\n",
    "validation_set_df_sampling = validation_set_df.copy()\n",
    "for i in range(10):\n",
    "    pos_sample = validation_set_df_pos.sample(n=20)\n",
    "    validation_set_df_sampling = validation_set_df_sampling.append(pos_sample)\n",
    "\n",
    "    neg_sample = validation_set_df_neg.sample(n=20)\n",
    "    validation_set_df_sampling = validation_set_df_sampling.append(neg_sample)\n",
    "np.random.shuffle(validation_set_df_sampling.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv format\n",
    "train_set_df.to_csv('train_set_df.csv', index=False)\n",
    "#train_set_df_sampling.to_csv('train_set_df.csv', index=False)\n",
    "validation_set_df.to_csv('validation_set_df.csv', index=False)\n",
    "#validation_set_df_sampling.to_csv('validation_set_df.csv', index=False)\n",
    "test_set_df.to_csv('test_set_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Embedding == 'W2V':\n",
    "    # Create word2vector model\n",
    "    W2V_model = Word2Vec(sentences=Train_word['word'], size=100, window=5, min_count=5, sg=0)\n",
    "    # Load W2V\n",
    "    W2V_model.wv.save_word2vec_format('w2v_model')\n",
    "    loaded_model = KeyedVectors.load_word2vec_format('w2v_model')\n",
    "    W2V_model = loaded_model\n",
    "    print('----- Save Word2Vector embedding model -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Torchtext structure\n",
    "TEXT = torchtext.data.Field(sequential=True, use_vocab=True,\n",
    "                            tokenize=str.split, lower=True,\n",
    "                            batch_first=True, fix_length=NUM_WORDS)\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=True,\n",
    "                            batch_first=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, valid_data, test_data = TabularDataset.splits(\n",
    "    path='.', train='train_set_df.csv', validation='validation_set_df.csv', test='test_set_df.csv',\n",
    "    format='csv', fields=[('label', LABEL), ('text', TEXT)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab with embedding vector\n",
    "if Embedding == 'W2V':\n",
    "    w2v_vectors = Vectors('w2v_model')\n",
    "    TEXT.build_vocab(train_data, vectors=w2v_vectors, min_freq=5)\n",
    "elif Embedding == 'G6B':\n",
    "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=100) , min_freq=5, max_size=10000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9310\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "print('Vocabulary Size: {}'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batch in train_data : 50\n",
      "The number of mini-batch in validation_data : 13\n"
     ]
    }
   ],
   "source": [
    "# Define data bucket and iterator\n",
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "                                        (train_data, valid_data),\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        sort_key=lambda x: len(x.text),\n",
    "                                        sort_within_batch = False,\n",
    "                                        shuffle=True, repeat=False,\n",
    "                                        device = device)\n",
    "\n",
    "print('The number of mini-batch in train_data : {}'.format(len(train_iter)))\n",
    "print('The number of mini-batch in validation_data : {}'.format(len(valid_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(9310, 100)\n",
       "  (rnn): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "INPUT_DIM = VOCAB_SIZE\n",
    "EMBEDDING_DIM = NUM_DIM\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = RNN(INPUT_DIM,\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            OUTPUT_DIM,\n",
    "            N_LAYERS,\n",
    "            DROPOUT)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding parameters\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model.embedding.weight.data[0] = (torch.rand(EMBEDDING_DIM)-0.5)*0.001 #<unk>\n",
    "model.embedding.weight.data[1] = torch.zeros(EMBEDDING_DIM) #<pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate Model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 10] valid loss : 0.680 | valid accuracy : 56.500\n",
      "[Epoch: 20] valid loss : 0.392 | valid accuracy : 84.500\n",
      "[Epoch: 30] valid loss : 0.403 | valid accuracy : 85.500\n",
      "[Epoch: 40] valid loss : 0.291 | valid accuracy : 90.375\n",
      "[Epoch: 50] valid loss : 0.253 | valid accuracy : 92.125\n",
      "CPU times: user 8min 33s, sys: 5.87 s, total: 8min 39s\n",
      "Wall time: 8min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_val_loss = None\n",
    "train_out = []\n",
    "valid_out = []\n",
    "test__out = []\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    train_loss, train_accuracy = train(model, optimizer, train_iter)\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_iter)\n",
    "\n",
    "    train_out.append([train_loss, train_accuracy])\n",
    "    valid_out.append([valid_loss, valid_accuracy])\n",
    "\n",
    "    if e%10==0:\n",
    "        #print(\"[Epoch: %d] train loss : %3.3f | train accuracy : %3.3f\" % (e, train_loss, train_accuracy))\n",
    "        print(\"[Epoch: %d] valid loss : %3.3f | valid accuracy : %3.3f\" % (e, valid_loss, valid_accuracy))\n",
    "\n",
    "    if not best_val_loss or valid_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/LSTM_classification.pt')\n",
    "        best_val_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Figure\n",
    "plt.figure()\n",
    "plt.plot(np.array(train_out)[:,0])\n",
    "plt.plot(np.array(valid_out)[:,0])\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.title('loss'+'_'+str(Embedding)+'_'+str(Model)+'_'+str(EPOCHS))\n",
    "plt.savefig('loss'+'_'+str(Embedding)+'_'+str(Model)+'_'+str(EPOCHS)+'.png')\n",
    "#plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.ylim((45,105))\n",
    "plt.plot(np.array(train_out)[:,1])\n",
    "plt.plot(np.array(valid_out)[:,1])\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.title('accuracy'+'_'+str(Embedding)+'_'+str(Model)+'_'+str(EPOCHS))\n",
    "plt.savefig('accuracy'+'_'+str(Embedding)+'_'+str(Model)+'_'+str(EPOCHS)+'.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./snapshot/LSTM_classification.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, eval_data):\n",
    "    eval_iter_pred = Iterator(eval_data, batch_size=len(eval_data),\n",
    "                              sort_key=lambda x: len(x.text),\n",
    "                              sort_within_batch = False,\n",
    "                              shuffle=False, repeat=False,\n",
    "                              device = device)    \n",
    "    batch = next(iter(eval_iter_pred))\n",
    "    x = batch.text.to(device)\n",
    "    logit = model(x)\n",
    "    pred_idx =logit.max(1)[1].data.tolist()\n",
    "    pred = np.array([LABEL.vocab.itos[idx+1] for idx in pred_idx])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Train score: 0.95969\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(model, train_data)\n",
    "y_train = np.array([data.label for data in train_data.examples])\n",
    "print(' LSTM Train score: {:.5f}'.format(np.sum(pred_train==y_train)/len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Train score: 0.93125\n"
     ]
    }
   ],
   "source": [
    "pred_valid = predict(model, valid_data)\n",
    "y_valid = np.array([data.label for data in valid_data.examples])\n",
    "print(' LSTM Train score: {:.5f}'.format(np.sum(pred_valid==y_valid)/len(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_heldout = predict(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved: heldout_pred_nn_G6B.txt\n"
     ]
    }
   ],
   "source": [
    "with open(heldout_pred_file, 'w') as f:\n",
    "    for l in pred_heldout:\n",
    "        f.write(str(l)+'\\n')\n",
    "    f.close()\n",
    "print('Prediction saved: {}'.format(heldout_pred_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_df)",
   "language": "python",
   "name": "conda_df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
